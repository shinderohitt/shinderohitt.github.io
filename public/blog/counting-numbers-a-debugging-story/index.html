<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Counting Numbers: A Debugging Story | Rohit Shinde</title>
<meta name="title" content="Counting Numbers: A Debugging Story" />
<meta name="description" content="A story about how we figured out issues in an ad system, and fixed it. This involved tools like Apache Kafka, Redis, BigQuery." />
<meta name="keywords" content="" />


<meta property="og:title" content="Counting Numbers: A Debugging Story" />
<meta property="og:description" content="A story about how we figured out issues in an ad system, and fixed it. This involved tools like Apache Kafka, Redis, BigQuery." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rohitshinde.in/blog/counting-numbers-a-debugging-story/" /><meta property="article:section" content="blog" />
<meta property="article:published_time" content="2023-01-08T10:21:26+05:30" />
<meta property="article:modified_time" content="2023-01-08T10:21:26+05:30" /><meta property="og:site_name" content="Rohit Shinde" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Counting Numbers: A Debugging Story"/>
<meta name="twitter:description" content="A story about how we figured out issues in an ad system, and fixed it. This involved tools like Apache Kafka, Redis, BigQuery."/>



<meta itemprop="name" content="Counting Numbers: A Debugging Story">
<meta itemprop="description" content="A story about how we figured out issues in an ad system, and fixed it. This involved tools like Apache Kafka, Redis, BigQuery."><meta itemprop="datePublished" content="2023-01-08T10:21:26+05:30" />
<meta itemprop="dateModified" content="2023-01-08T10:21:26+05:30" />
<meta itemprop="wordCount" content="1980">
<meta itemprop="keywords" content="" />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
 body {
    font-family: Verdana, sans-serif;
    margin: auto;
    padding: 20px;
    max-width: 720px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #3273dc;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
    margin: 0;
  }


  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }

  

 .twitter {
     border-top: 1px solid #ccc;
     font-size: 18px;
 }
</style>


<script>
 window.dataLayer = window.dataLayer || [];
 function gtag(){dataLayer.push(arguments);}
 gtag('js', new Date());

 gtag('config', 'UA-104099613-1');
</script>

<style>
 body {
     background-color: white;
     font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol"
     font-size: 17px;
     line-height: 24px;
     line-height: 1.4;
      
      
 }

  
 .full-width {
     width: 95vw;
     position: relative;
     left: 55%;
     right: 50%;
     margin-left: -50vw;
     margin-right: -50vw;
 }

 h1, h2, h3, h4, h5, h6, strong, b {
     color: #454545;
 }

 blockquote {
     border: 1px solid #dfe7fd;
     border-radius: 7px;
     font-style: normal;
     background-color: #f8fffc;
 }

 a {
     text-decoration: none;
     color: #0a84ff;
 }

 a:hover {
     text-decoration: underline;
 }

 ul li {
     margin-bottom: 10px;
 }

 ol {
     list-style-type: decimal;
 }
</style>
<meta content="index, follow" name="robots">
</head>

<body>
  <header><a href="/" class="title">
    
</a>
<nav>




</nav>
</header>
  <main>

<h4><a href="/">Rohit Shinde</a></h2>
<h1>Counting Numbers: A Debugging Story</h1>
<p>
  <i>
    <time datetime='2023-01-08' pubdate>
      08 Jan, 2023
    </time>
  </i>
</p>

<content>
  <h3 id="what-is-ads">What is ads?</h3>
<p>In an ad system, there are three main entities. A business which wants to show an ad to people, the group of people who are eligible to see that ad, and the platform which ties both the parties. The platform generally charges money to businesses based on the number of views, or clicks the ad gets.</p>
<p><a href="https://www.gotocompany.com/">Goto</a> is a fat app, and serves many use-cases. You can order food, a ride, or pay for things, and more. The app serves a lot of users (10s of millions). The ads team at Goto monetizes some of the pages within the app by serving ads.</p>
<h3 id="why-do-we-need-to-count-the-numbers">Why do we need to count the numbers?</h3>
<p>How do you charge money? It&rsquo;s based on <em>the number of views</em> the platform can get for an ad. That involves counting numbers. A business asks a simple question, &ldquo;How many people saw my ad campaign today?&rdquo;. Well, it&rsquo;s a number, and the platform needs to keep a counter against that ad.</p>
<h3 id="so-what-was-the-problem">So what was the problem?</h3>
<p>When I joined the ad team, an issue was brought to tech team&rsquo;s attention: Some campaigns were not getting enough impressions (starving), while others were getting more than necessary. More than necessary means the platform is losing money. The same view could have been served to the starving campagin and there would be more money.</p>
<h4 id="some-lingo">Some lingo</h4>
<p>Before we proceed further, let me set up some terms. Maybe refer these whenever confused because of an unfamiliar term.</p>
<ul>
<li><code>Impression</code>: An event that&rsquo;s generated against an ad campaign when a user sees the ad, or clicks on it</li>
<li><code>Units purchased</code>: When a business creates an ad, they buy a fixed number of impressions. Something like, &ldquo;Hey platform, give me 10,000 views today for my campaign X&rdquo;</li>
<li><code>Campaign starvation</code>: A campagin is getting way less impressions than the units purchased.</li>
<li><code>Inventory utilisation</code>: Let&rsquo;s say a page in the app can generate 30 million events in a day, how many of these views are generating money for the platform? $Utilisation = \frac{x}{y}$</li>
<li>I may be using the words &ldquo;ad&rdquo;, &ldquo;ad campaign&rdquo;, or &ldquo;campaign&rdquo; interchangeably. They mean the same thing.</li>
</ul>
<h3 id="architecture">Architecture</h3>
<p>Now before we point out issues with the setup, lets quickly paint what system looked like.</p>
<div style="display: flex; justify-content: center;">
    <img width="50%" src="/images/counting-numbers-a-debugging-story/high-level.svg" />
</div>
<p>As described above, the communication between three entities is captured by this high level diagram above. Below is a detailed architecture. Henceforth, we will ignore the campagin creation flow and focus on impression counting and campaign serving.</p>
<img src="/images/counting-numbers-a-debugging-story/old-architecture.svg" />
<p>So, what&rsquo;s happening in this?</p>
<ol>
<li>The mobile app used by consumers send an event (view, or click ~ impression) to an internal service called Clickstream.</li>
<li>This service puts these events on a kafka topic. Let&rsquo;s call it topic 1. Now the app is huge, and all sorts of events are recorded, all of them end up on topic 1. The ads impressions are a subset of them.</li>
<li><a href="https://odpf.github.io/dagger/">Dagger</a> essentially acted as a filter on top of topic 1. It picked the relevant impression events and put them on another kafka topic. Let&rsquo;s call it topic 2.</li>
<li>The consumer on topic 2 picked up these events, and kept a time window against each user session and aggregated counts against each campaign (more on this below).</li>
<li>The redis box is where the state of <code>campaign: impression counts &lt;number&gt;</code> state lived.</li>
</ol>
<p>Here is how the event message (on kafka) looked like:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-javascript" data-lang="javascript"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">campaign_id</span><span style="color:#f92672">:</span> <span style="color:#f92672">&lt;</span><span style="color:#a6e22e">UUID</span><span style="color:#f92672">&gt;</span>, <span style="color:#75715e">// To identify the campaign against which this was generated
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#a6e22e">user_id</span><span style="color:#f92672">:</span> <span style="color:#f92672">&lt;</span><span style="color:#a6e22e">UUID</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">session_id</span><span style="color:#f92672">:</span> <span style="color:#f92672">&lt;</span><span style="color:#a6e22e">UUID</span><span style="color:#f92672">&gt;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#a6e22e">event_type</span><span style="color:#f92672">:</span> <span style="color:#f92672">&lt;</span><span style="color:#e6db74">&#34;CLICK&#34;</span> <span style="color:#f92672">|</span> <span style="color:#e6db74">&#34;VIEW&#34;</span><span style="color:#f92672">&gt;</span>, <span style="color:#75715e">// Type of the impression
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#a6e22e">event_timestamp</span><span style="color:#f92672">:</span> <span style="color:#a6e22e">unix_epoch</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h3 id="what-is-time-window-point-4-above">What is time window? (point 4 above)</h3>
<ul>
<li>If the same user sees the same ad twice within a 40 second window, we had to count that as once.</li>
<li>The window size is different for clicks, and views. The idea is roughly the same.</li>
</ul>
<h3 id="how-did-counts-work">How did counts work?</h3>
<p>This will mostly talk about the consumer on topic 2.</p>
<ol>
<li>Pick a batch of events</li>
<li>Generate a list of keys for every message (to deduplicate events which fall within 40 seconds). Let&rsquo;s call  them <em>dedup keys</em>. This was essentially a function of: <code>campaign_id + user_id + event_type + (event_timestamp % 40)</code></li>
<li>Now check which of these dedup keys exist in Redis.</li>
<li>Now filter out those campaign_ids from the original batch of messages, whose dedup keys exist in Redis.</li>
<li>For filtered campaigns, make another batch call to Redis, and increment counts for them.</li>
<li>Now for the same list of campaign_ids, set dedup keys in Redis - so if you see another event which falls within the same window, we don&rsquo;t count that twice (step 4 in this list).</li>
</ol>
<h3 id="back-to-the-problem">Back to the problem</h3>
<ul>
<li>Why was the system serving some campaigns 100s of thousands of times even though it had asked for only 15k events?</li>
<li>Why some campaigns were getting zero impressions?</li>
</ul>
<p>The overall effect of this was that the effective utilisation rate of the ads was ~50%. Which should have been at least 85% or above.</p>
<ul>
<li>First thing we noticed was that the lag on topic 1 was huge. At peak, it reached 300 million. At night the consumer kept chugging at it, and by morning it would come down to zero. But as the day progressed, it would grow again and reach 100s of millions. It essentially looked like a sine wave for days of data.</li>
<li>What would be the impact of this lag on the system? Essentially you are counting impressions later than real time.</li>
<li>Let&rsquo;s say there&rsquo;s a campaign which has purchased 15k impressions for the day.</li>
<li>At 10am, it has already seen those impressions. But the system is lagging behind, it doesn&rsquo;t yet know that the campaign has been served all of its impressions and it should be deactived. The impact of deactivating that campaign is, some other campaign would take its spot.</li>
<li>The delay essentially meant we are inefficient in making all the money out of the ad page that we could. We can&rsquo;t charge a business for 300k impressions if they&rsquo;ve only asked for 15k. Those 285k impressions could&rsquo;ve been served to the other campaign which is still at zero.</li>
</ul>
<h3 id="to-fix-the-lag">To fix the lag</h3>
<ul>
<li>We increased the number of threads on Dagger, and the lag on topic 1 started going down. But the side effect was it started building up on topic 2. The Redis could not keep up with the increased load.</li>
<li>To fix that, we introduced more Redis boxes and partitioned the data on campaign_id. The change looked like this:</li>
</ul>
<img src="/images/counting-numbers-a-debugging-story/new-architecture.svg" />
<ul>
<li>After this the lag on both topics went down. Everything was normal, all dashboards green.</li>
<li>Yet, the problem still persisted. There was no impact on the utilisation.</li>
<li>The mystery forced us to look at data, and the two points that stood out were:</li>
</ul>
<blockquote>
<p>The average number of impressions a campaign could receive in 1 minute: 15,000
The average number of impressions a campaign purchased for a day: 16,000</p>
</blockquote>
<p>So roughly the same numbers. What it means is you could burn through all of campaign&rsquo;s impressions in a minute - this would be the worst case, there were other complexities that decided which campaign to serve. We&rsquo;re not talking about them here.</p>
<h3 id="is-your-cron-running-frequent-enough">Is your cron running frequent enough?</h3>
<ul>
<li>We had to deactivate the campagins which have already received enough impressions. That was done by a cron job periodically.</li>
<li>For all active campaigns, it fetched the current impression count from Redis. If the count was more than units purchased, it deactivated the campagin by updating its <code>active</code> flag to <code>false</code> in ElasticSearch.
<img src="/images/counting-numbers-a-debugging-story/cron.svg" /></li>
<li>And so the campaign would start showing up and getting any more impressions.</li>
<li>What would happen if this job is running at every 30th minute? (Refer the numbers above)</li>
<li>In the worst case, we let the campaign be served for 30 minutes even after it has received enough impressions already!</li>
<li>So in terms of numbers: a campaign asking for 15k impressions could potentially get 15,000 x 30 = 450,000 impressions!</li>
<li>We immediately reduced the frequency to 1 minute.</li>
<li>This finally gave us some positive results, but there was still not ideal.</li>
<li>Another optimisation was to deactivate campaigns prematurely, lets say at 70% instead of 100% of fulfilment. Imagine a campaign having received 12k events at 59th second. The cron runs, and does not deactivate the campaign. Now it gets visibility for another minute. That can potentially add another 15k impressions. So even though the campaign is asking for 15k impressions in total, we give it 27k.</li>
</ul>
<h3 id="is-the-ttl-long-enough">Is the TTL long enough?</h3>
<ul>
<li>We had to set a TTL on the dedup keys I talked about above. For an ever increasing data, a cache without TTL (or some eviction policy) would run out of memory.</li>
<li>We found out the TTL for that was set to 40 seconds - same as the window size!</li>
<li>In this case, if the aggregator saw an event even one second late, it would count that event twice! Let me explain:</li>
</ul>
<img src="/images/counting-numbers-a-debugging-story/timeline.svg" />
<ul>
<li>As soon as you see an event, you mark that with its dedup key in Redis. You set it to expire after 40 seconds.</li>
<li>If you continue to see more events within those 40 seconds, they are ignored.</li>
<li>If you see an event that crosses that boundary of 40 seconds, you would count that event twice. This may be a bit confusing to understand because, isn&rsquo;t that not what we want to do? keep a 40 second window? It&rsquo;s correct, but the events can be delayed.</li>
<li>Assume all green lines above are events generated within the same 40 second time window, but the gap between first event and the last event was wider than 40 seconds: All of the events that fall within 40 seconds of the first event would be counted as 1 + the last event because it crosses the boundary. Ideally all of them should be 1.</li>
<li>To fix this, we increased the TTL to 30 minutes. This provided a good enough tolerance for delayed events.</li>
<li>This issue was a bit sneaky, and the impact wasn&rsquo;t directly seen. This essentially ballooned impressions for some systems in Redis, but on BQ they were lower. This change brought a significant improvement in bringing parity with BQ numbers.</li>
</ul>
<h3 id="get-rid-of-the-ttl-make-counters-idempotent---hyperloglog">Get rid of the TTL, make counters idempotent - HyperLogLog</h3>
<blockquote>
<p>Unique items can be difficult to count. Usually this means storing every unique item then recalling this information somehow. With Redis, this can be accomplished by using a set and a single command, however both the storage and time complexity of this with very large sets is prohibitive. HyperLogLog provides a probabilistic alternative.</p>
</blockquote>
<ul>
<li>This allowed us moving away from the <em>dedup key</em>, so there was no question of expiring it with some TTL.</li>
<li>Each operation became idempotent. You can&rsquo;t add the same item to a set multiple times. The set would deduplicate it for you.</li>
<li>We only had to use two HyperLogLog commands to get this done:
<ul>
<li><code>PFADD &lt;campaign_id:date&gt; &lt;dedup key&gt;</code> (dedup key is exactly same as above). This increments the cardinality of the set, if the dedup key is the same. Recall that all impression events whose event_timestamp falls in the same 40s window</li>
<li><code>PFCOUNT &lt;campaign_id:date&gt;</code> To get the impressions against a campaign for a given day.</li>
</ul>
</li>
<li>With this, we reduced the calls to Redis by 2/3. From checking dedup keys, incrementing counters, updating dedup keys to only incrementing counters.</li>
<li>HLL spec: The Redis HyperLogLog implementation uses up to 12 KB and provides a standard error of 0.81%. Antirez has <a href="http://antirez.com/news/75">an article</a> about it, links to papers.</li>
</ul>
<h3 id="summary-and-conclusion">Summary and conclusion</h3>
<ul>
<li>This involved figuring out kafka topic delays and their impact, figuring out bottlenecks that were involved in increasing event consumption.</li>
<li>Looking at data and figuring out the average number of impressions a campaign purchased for a day, and the potential number of impressions a campaign could get in a minute.</li>
<li>Figuring out cron&rsquo;s frequency and its relation with overserving, or underserving.</li>
<li>Understanding TTL, and its impact. Prematurely deactivating campaigns.</li>
<li>Experimenting with and migrating to a setup using HyperLogLog.</li>
</ul>

</content>
<p>
  
</p>

  </main>
  <footer>
</footer>

    
</body>

</html>
